{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3998a4-5bbc-4a1d-b4dd-0bafb1e4aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import unittest\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980f40c4-20e8-48b9-b3c2-cb03666c22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = pd.read_csv(\"test_dataset.csv\")\n",
    "new_dataset = original_dataset.assign(without_object_base=lambda x: '', without_object_polypers=lambda x: '')\n",
    "\n",
    "changed_subset = new_dataset[new_dataset[\"was_changed\"]].reset_index(drop=True)\n",
    "unchanged_subset = new_dataset[~new_dataset[\"was_changed\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d0ff2b-7c02-4ac2-afe1-e0223d60ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "disabled_comps = ['ner', 'senter', 'attribute_ruler', 'lemmatizer', 'morphologizer']\n",
    "model = spacy.load(\"ru_core_news_lg\", disable=disabled_comps)\n",
    "morph = MorphAnalyzer().parse\n",
    "\n",
    "list_to_avoid = ['ее', 'её', 'его', 'их']\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, base, poly):\n",
    "        self.parsed_sent = base\n",
    "        self.parsed_poly = poly\n",
    "\n",
    "    def find_object_and_children(self):\n",
    "        objects_children = list()\n",
    "        objects_list = list()\n",
    "        for token in self.parsed_sent:\n",
    "            if token.dep_ == 'obj':\n",
    "                objects_list.append(token)\n",
    "                objects_children += [child for child in token.children if child.dep_ != 'conj' or child.dep_ != 'parataxis']\n",
    "            elif token.dep_ == 'conj' or token.dep_ == 'parataxis':\n",
    "                for ancestor in token.ancestors:\n",
    "                    if ancestor.dep_ == 'obj':\n",
    "                        objects_list.append(token)\n",
    "                        objects_children += [child for child in token.children if child.dep_ != 'conj' or child.dep_ != 'parataxis']\n",
    "            \n",
    "        return objects_list, objects_children\n",
    "\n",
    "\n",
    "    def child_recursion(self, token):\n",
    "        all_children = [child for child in token.children if child.pos_ != 'PUNCT']\n",
    "        if all_children == []:\n",
    "            return all_children\n",
    "        else:\n",
    "            for child in all_children:\n",
    "                all_children += self.child_recursion(child)\n",
    "        return set(all_children)\n",
    "\n",
    "\n",
    "    def if_coordinated(self, token):\n",
    "        parsed_word = morph(token.text)\n",
    "        all_tags = set(itertools.chain.from_iterable([re.split(r',| ', str(parsed_word[i].tag)) for i in range(len(parsed_word))]))\n",
    "        stopped = False\n",
    "        for tag in all_tags:\n",
    "            if tag == 'ADJF' or tag == 'PRTF':\n",
    "                stopped = True\n",
    "                break\n",
    "        return stopped\n",
    "\n",
    "    \n",
    "    def construct_sentence(self):\n",
    "        objects_list, object_children = self.find_object_and_children()\n",
    "        \n",
    "        res_sent = dict()\n",
    "        add_stops = list()\n",
    "        for token in self.parsed_sent:\n",
    "            if token not in objects_list and token not in object_children:\n",
    "                res_sent[token] = 'PASS'\n",
    "            elif token not in objects_list and token in object_children:\n",
    "                if self.if_coordinated(token) == True and token.text not in list_to_avoid:\n",
    "                    res_sent[token] = 'STOP'\n",
    "                    add_stops += self.child_recursion(token)\n",
    "                else:\n",
    "                    res_sent[token] = 'PASS'\n",
    "            elif token in objects_list:\n",
    "                 res_sent[token] = 'STOP'\n",
    "\n",
    "        for token in self.parsed_sent:\n",
    "            if token in add_stops and self.if_coordinated(token) == True and token.text not in list_to_avoid:\n",
    "                 res_sent[token] = 'STOP'\n",
    "\n",
    "        res_sent_poly = dict()\n",
    "        for token in self.parsed_poly:\n",
    "            res_sent_poly[token] = 'poly'\n",
    "     \n",
    "        return res_sent, res_sent_poly\n",
    "\n",
    "    \n",
    "    def form_sentence(self):\n",
    "        res = list()\n",
    "        prev_mask = False\n",
    "        punct = False\n",
    "        parsed_sent, poly_parsed_sent = self.construct_sentence()\n",
    "        for word in parsed_sent.keys():\n",
    "            if parsed_sent[word] == 'PASS':\n",
    "                res.append(word.text)\n",
    "                prev_mask = False\n",
    "            elif parsed_sent[word] == 'STOP':\n",
    "                if prev_mask == False:\n",
    "                    res.append('MASK')\n",
    "                    prev_mask = True\n",
    "\n",
    "        res_poly = list()\n",
    "        prev_mask = False\n",
    "        for num in range(len(parsed_sent)):\n",
    "            if list(parsed_sent.keys())[num].text != list(poly_parsed_sent.keys())[num].text:\n",
    "                res_poly.append(list(poly_parsed_sent.keys())[num].text)\n",
    "            else:\n",
    "                if parsed_sent[list(parsed_sent.keys())[num]] == 'PASS':\n",
    "                    res_poly.append(list(parsed_sent.keys())[num].text)\n",
    "                    prev_mask = False\n",
    "                elif parsed_sent[list(parsed_sent.keys())[num]] == 'STOP':\n",
    "                    if prev_mask == False:\n",
    "                        res_poly.append('MASK')\n",
    "                        prev_mask = True\n",
    "\n",
    "        # print(len(parsed_sent) == len(poly_parsed_sent))\n",
    "        return {'BASE': re.sub(r'Ы', '-', ' '.join(res)), 'POLY': re.sub(r'Ы', '-', ' '.join(res_poly))}\n",
    "\n",
    "\n",
    "    def show_scheme(self):\n",
    "        displacy.render(self.parsed_sent, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7789130-bf07-42b8-9727-1b1d44949ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing base sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59801/59801 [00:21<00:00, 2719.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing polypersonal sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59801/59801 [00:18<00:00, 3302.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59801/59801 [02:09<00:00, 462.14it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_base = changed_subset['base'].str.replace(r'(?<=[a-zA-Zа-яА-Я])-(?=[a-zA-Zа-яА-Я])', 'Ы', regex=True)\n",
    "sentences_poly = changed_subset['polypers'].str.replace(r'(?<=[a-zA-Zа-яА-Я])-(?=[a-zA-Zа-яА-Я])', 'Ы', regex=True)\n",
    "\n",
    "print(\"Processing base sentences\")\n",
    "parsed_sentences_base = list(tqdm(model.pipe(sentences_base), total=len(changed_subset)))\n",
    "\n",
    "print(\"Processing polypersonal sentences\")\n",
    "parsed_sentences_poly = list(tqdm(model.pipe(sentences_poly), total=len(changed_subset)))\n",
    "\n",
    "print(\"Deleting objects\")\n",
    "for i in tqdm(range(len(changed_subset))):\n",
    "    sentence = Sentence(parsed_sentences_base[i], parsed_sentences_poly[i])\n",
    "    changed_subset.loc[i, 'without_object_base'] = sentence.form_sentence()['BASE']\n",
    "    changed_subset.loc[i, 'without_object_polypers'] = sentence.form_sentence()['POLY']\n",
    "\n",
    "unchanged_subset['without_object_base'] = unchanged_subset['base']\n",
    "unchanged_subset['without_object_polypers'] = unchanged_subset['polypers']\n",
    "\n",
    "result_dataset = pd.concat([changed_subset, unchanged_subset]).sort_values(by='Unnamed: 0')\n",
    "result_dataset.to_csv(\"result_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c631680",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset = pd.read_csv(\"result_dataset.csv\")\n",
    "result_dataset = result_dataset[result_dataset[\"was_changed\"]].reset_index(drop=True)\n",
    "\n",
    "with codecs.open('check.txt', 'w', 'utf-8') as f:\n",
    "    for i in range(len(result_dataset)):\n",
    "        if result_dataset['was_changed'][i] ==True:\n",
    "            f.write(f'{i}_Polypers: {result_dataset[\"polypers\"][i]}\\n')\n",
    "            f.write(f'{i}_Polypers_mod: {result_dataset[\"without_object_polypers\"][i]}\\n')\n",
    "            f.write(f'{i}_Base: {result_dataset[\"base\"][i]}\\n')\n",
    "            f.write(f'{i}_Base_mod: {result_dataset[\"without_object_base\"][i]}\\n')\n",
    "            f.write('________________\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7487cb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old = pd.read_csv(\"result_dataset_old.csv\").sort_values(by='Unnamed: 0').reset_index(drop=True)\n",
    "df_new = pd.read_csv(\"result_dataset.csv\").sort_values(by='Unnamed: 0').reset_index(drop=True)\n",
    "df_old.compare(df_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "albert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
