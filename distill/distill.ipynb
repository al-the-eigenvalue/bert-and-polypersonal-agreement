{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, RobertaModel, RobertaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(device)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "_wy4FMYDWlUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = AutoModelForCausalLM.from_pretrained(\"FacebookAI/roberta-base\", is_decoder=True)\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
      ],
      "metadata": {
        "id": "ycSEqJ5atDeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1 = gpt2_tokenizer.get_vocab()\n",
        "vocab2 = bert_tokenizer.get_vocab()"
      ],
      "metadata": {
        "id": "o4Ln4r42k8CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_tokens = set.intersection(set([*vocab1.keys()]), set([*vocab2.keys()]))"
      ],
      "metadata": {
        "id": "whGorQhvh16d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab1_keys = set(gpt2_tokenizer.get_vocab().keys())\n",
        "vocab2_keys = set(bert_tokenizer.get_vocab().keys())\n",
        "intersection = vocab1_keys & vocab2_keys\n",
        "print(len(intersection) <= min(len(vocab1_keys), len(vocab2_keys)))  # Must be True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV12O0AlrKMO",
        "outputId": "5c450ab2-f8bc-49b8-abfe-ebba30ac5e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_mapping = {}\n",
        "for token in common_tokens:\n",
        "    id_mapping[vocab1[token]] = vocab2[token]"
      ],
      "metadata": {
        "id": "3LSw84pziMH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset ="
      ],
      "metadata": {
        "id": "V7r8SPKuZ-Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# итеративно\n",
        "def collate_fn(batch, max_length=64, window_size=3):\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "    gpt_inputs = gpt2_tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    bert_inputs = bert_tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = bert_inputs[\"input_ids\"]\n",
        "    attention_mask = bert_inputs[\"attention_mask\"]\n",
        "\n",
        "    all_bert_inputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(input_ids.size(0)):\n",
        "        non_pad = (input_ids[i] != bert_tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
        "        seq_len = len(non_pad)\n",
        "\n",
        "        # несколько маскирований, передвигая окно\n",
        "        for pos in range(seq_len - window_size + 1):\n",
        "            masked_input = input_ids[i].clone()\n",
        "            labels = torch.full_like(masked_input, -100)\n",
        "\n",
        "            # маскирование текущего окна\n",
        "            mask_indices = non_pad[pos:pos+window_size]\n",
        "            labels[mask_indices] = masked_input[mask_indices]\n",
        "            masked_input[mask_indices] = bert_tokenizer.mask_token_id\n",
        "\n",
        "            all_bert_inputs.append(masked_input)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    # stack все маскирования\n",
        "    if len(all_bert_inputs) > 0:\n",
        "        bert_input_ids = torch.stack(all_bert_inputs)\n",
        "        bert_labels = torch.stack(all_labels)\n",
        "\n",
        "        # attention_mask для каждого маскирования\n",
        "        bert_attention_mask = attention_mask.repeat(len(all_bert_inputs) // attention_mask.size(0), 1)\n",
        "\n",
        "        # для gpt\n",
        "        gpt_input_ids = gpt_inputs[\"input_ids\"].repeat(len(all_bert_inputs) // gpt_inputs[\"input_ids\"].size(0), 1)\n",
        "        gpt_attention_mask = gpt_inputs[\"attention_mask\"].repeat(len(all_bert_inputs) // gpt_inputs[\"attention_mask\"].size(0), 1)\n",
        "\n",
        "    else:\n",
        "        # если последовательность меньше окна\n",
        "        bert_input_ids = input_ids\n",
        "        bert_labels = torch.full_like(input_ids, -100)\n",
        "        bert_attention_mask = attention_mask\n",
        "        gpt_input_ids = gpt_inputs[\"input_ids\"]\n",
        "        gpt_attention_mask = gpt_inputs[\"attention_mask\"]\n",
        "\n",
        "    return {\n",
        "        \"gpt_input_ids\": gpt_input_ids.to(device),\n",
        "        \"gpt_attention_mask\": gpt_attention_mask.to(device),\n",
        "        \"bert_input_ids\": bert_input_ids.to(device),\n",
        "        \"bert_attention_mask\": bert_attention_mask.to(device),\n",
        "        \"bert_labels\": bert_labels.to(device)\n",
        "    }"
      ],
      "metadata": {
        "id": "qrodjW_zyajv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
      ],
      "metadata": {
        "id": "UHklCVw6aLjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    bert_model.train()\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "    for batch in pbar:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gpt_logits = gpt2_model(input_ids=batch[\"gpt_input_ids\"], attention_mask=batch[\"gpt_attention_mask\"]).logits\n",
        "\n",
        "        bert_logits = bert_model(input_ids=batch[\"bert_input_ids\"], attention_mask=batch[\"bert_attention_mask\"])\n",
        "\n",
        "        loss_mask = (batch[\"bert_labels\"] != -100)\n",
        "        loss = criterion(bert_logits[loss_mask].view(-1, gpt2_model.config.vocab_size), gpt_logits[loss_mask].argmax(dim=-1).view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        pbar.set_postfix({\"loss\": loss.item()})"
      ],
      "metadata": {
        "id": "oxroY4hpagWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.save_pretrained(\"\")"
      ],
      "metadata": {
        "id": "UAz-YU1aaudL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}